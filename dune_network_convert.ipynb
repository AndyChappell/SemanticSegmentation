{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"dune_network_convert.ipynb","provenance":[{"file_id":"1VyLoAXUy2Rd85E9752fd5yi39lu0-JB_","timestamp":1573658945097},{"file_id":"1E1qOvI9tzgUCpptOmZo4z8Pi4GMz0keS","timestamp":1573636007501}],"collapsed_sections":["1EgZ6jJpDLld"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"code","metadata":{"id":"-VsDVF_pygLV","colab_type":"code","outputId":"a2477fd7-ede3-4f7d-db2e-28e493be32a3","executionInfo":{"status":"ok","timestamp":1580827116854,"user_tz":0,"elapsed":54084,"user":{"displayName":"Andy Chappell","photoUrl":"https://lh4.googleusercontent.com/-TuFiExc5DlA/AAAAAAAAAAI/AAAAAAAAIWA/grw-7ZxGJac/s64/photo.jpg","userId":"00288272189243102020"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["!pip uninstall torchvision\n","!pip uninstall torch"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Uninstalling torchvision-0.5.0:\n","  Would remove:\n","    /usr/local/lib/python3.6/dist-packages/torchvision-0.5.0.dist-info/*\n","    /usr/local/lib/python3.6/dist-packages/torchvision/*\n","Proceed (y/n)? y\n","  Successfully uninstalled torchvision-0.5.0\n","Uninstalling torch-1.4.0:\n","  Would remove:\n","    /usr/local/bin/convert-caffe2-to-onnx\n","    /usr/local/bin/convert-onnx-to-caffe2\n","    /usr/local/lib/python3.6/dist-packages/caffe2/*\n","    /usr/local/lib/python3.6/dist-packages/torch-1.4.0.dist-info/*\n","    /usr/local/lib/python3.6/dist-packages/torch/*\n","Proceed (y/n)? y\n","  Successfully uninstalled torch-1.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ke7aiSkToj6r","colab_type":"code","outputId":"dd1ce833-63fa-48f0-f26c-06a1d8d456d4","executionInfo":{"status":"ok","timestamp":1580827184093,"user_tz":0,"elapsed":62193,"user":{"displayName":"Andy Chappell","photoUrl":"https://lh4.googleusercontent.com/-TuFiExc5DlA/AAAAAAAAAAI/AAAAAAAAIWA/grw-7ZxGJac/s64/photo.jpg","userId":"00288272189243102020"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!pip install -q torch==1.0.1 torchvision==0.2.1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 560.1MB 31kB/s \n","\u001b[K     |████████████████████████████████| 61kB 10.0MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vVoe9tiw0Bg0","colab_type":"code","outputId":"b689f6a3-05e3-4046-f674-50f4da241efa","executionInfo":{"status":"ok","timestamp":1581321141258,"user_tz":0,"elapsed":3899,"user":{"displayName":"Andy Chappell","photoUrl":"https://lh4.googleusercontent.com/-TuFiExc5DlA/AAAAAAAAAAI/AAAAAAAAIWA/grw-7ZxGJac/s64/photo.jpg","userId":"00288272189243102020"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import torch\n","print(torch.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["1.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2hvEVBl1FV7i","colab":{}},"source":["# Automatically reload external libraries that change\n","%reload_ext autoreload\n","%autoreload 2\n","\n","# If a matplotlib plot command is issued, display the results in the notebook\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ReHf2y2V8jM","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","the_seed = 42\n","def set_seed(seed):\n","    # Note, deterministic can impede performance\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sOeYUQjj3rVz","colab_type":"code","outputId":"4a85f365-0684-4e10-fd36-59fb6ec5a2df","executionInfo":{"status":"ok","timestamp":1581321826899,"user_tz":0,"elapsed":103846,"user":{"displayName":"Andy Chappell","photoUrl":"https://lh4.googleusercontent.com/-TuFiExc5DlA/AAAAAAAAAAI/AAAAAAAAIWA/grw-7ZxGJac/s64/photo.jpg","userId":"00288272189243102020"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":108,"output_embedded_package_id":"1gvrdkOLmKUkayYoioJyZ4ZEp4M046ozP"}},"source":["# Upload data\n","from google.colab import files\n","files.upload()"],"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"wFjEqklm8e8g","colab_type":"code","colab":{}},"source":["!tar xzf images_w_hits.tar.gz\n","!tar xzf images_w_truth.tar.gz"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1EgZ6jJpDLld","colab_type":"text"},"source":["# Upload python files and import"]},{"cell_type":"code","metadata":{"id":"tvkscBA1bQ_J","colab_type":"code","outputId":"45e1d038-f690-4a6a-b3f7-0847146e0fc3","executionInfo":{"status":"ok","timestamp":1580827372633,"user_tz":0,"elapsed":44894,"user":{"displayName":"Andy Chappell","photoUrl":"https://lh4.googleusercontent.com/-TuFiExc5DlA/AAAAAAAAAAI/AAAAAAAAIWA/grw-7ZxGJac/s64/photo.jpg","userId":"00288272189243102020"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":363}},"source":["# Upload supporting python modules\n","!rm -f *.py\n","from google.colab import files\n","files.upload()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-80bf84c3-6c05-43e0-bf8c-e884ccaf1628\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-80bf84c3-6c05-43e0-bf8c-e884ccaf1628\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving data.py to data.py\n","Saving img_util.py to img_util.py\n","Saving model_util.py to model_util.py\n","Saving transforms.py to transforms.py\n","Saving unet_learner.py to unet_learner.py\n","Saving unet.py to unet.py\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'data.py': b'import numpy as np\\nimport torch\\nimport os\\nimport PIL\\nfrom torch.utils.data import Dataset\\nfrom torch.utils.data import DataLoader\\n\\ndef open_image(path):\\n    img = PIL.Image.open(path)\\n    if img.mode  != \\'L\\':\\n        img = img.convert(\\'L\\')\\n    return img\\n\\nclass SegmentationDataset(Dataset):\\n    \"\"\"Dataset suitable for segmentation tasks.\"\"\"\\n\\n    def __init__(self, image_dir, mask_dir, filenames, transform=None):\\n        \"\"\"\\n        Args:\\n            root_dir (string): Directory with all images and masks.\\n            image_dir (string): The relative directory containing the images.\\n            mask_dir (string): The relative directory containing the masks.\\n            transform (callable, optional): Optional transform to be applied\\n                on a sample.\\n        \"\"\"\\n        self.image_dir = image_dir\\n        self.mask_dir = mask_dir\\n        self.transform = transform\\n        self.filenames = filenames\\n        self.mean = 0.\\n        self.std = 255.\\n        self.normalise = False\\n    \\n    def set_image_stats(self, mean, std):\\n        self.mean = mean\\n        self.std = std\\n    \\n    def set_normalisation(self, norm=True):\\n        self.normalise = norm\\n\\n    def __len__(self):\\n        return len(self.filenames)\\n\\n    def __getitem__(self, idx):\\n        if torch.is_tensor(idx):\\n            idx = idx.tolist()\\n\\n        img_name = os.path.join(self.image_dir, self.filenames[idx])\\n        image = np.asarray(open_image(img_name)).astype(np.float32)\\n        if self.normalise:\\n            image -= self.mean\\n            image /= self.std\\n\\n        mask_name = os.path.join(self.mask_dir, self.filenames[idx])\\n        # If using categorical cross entropy, need an un-normalised long\\n        mask = np.asarray(open_image(mask_name)).astype(np.int_)\\n        sample = (image, mask)\\n\\n        if self.transform:\\n            sample = self.transform(sample)\\n\\n        return sample\\n\\nclass SegmentationBunch():\\n    \"\"\"Associates batches of training, validation and testing datasets suitable\\n    for segmentation tasks.\"\"\"\\n    def __init__(self, root_dir, image_dir, mask_dir, batch_size, valid_pct=0.1,\\n                 test_pct=0.0, transform=None):\\n        assert((valid_pct + test_pct) < 1.)\\n        image_dir = os.path.join(root_dir, image_dir)\\n        mask_dir = os.path.join(root_dir, mask_dir)\\n        transform = transform\\n        image_filenames = next(os.walk(image_dir))[2]\\n        random_list = np.random.choice(image_filenames, len(image_filenames),\\n                                       replace=False)\\n        valid_size = int(len(image_filenames) * valid_pct)\\n        test_size = int(len(image_filenames) * test_pct)\\n        train_size = len(image_filenames) - (valid_size + test_size)\\n        train_filenames = random_list[:train_size]\\n        valid_filenames = random_list[train_size:train_size + valid_size]\\n        self.valid_filenames = valid_filenames\\n        \\n        train_ds = SegmentationDataset(image_dir, mask_dir, train_filenames,\\n            transform)\\n        valid_ds = SegmentationDataset(image_dir, mask_dir, valid_filenames,\\n            transform)\\n        \\n        mu = 0.0\\n        for img, _ in train_ds:\\n            mu += torch.mean(img)\\n        mu /= len(train_ds)\\n        var_diff = 0.0\\n        for img, _ in train_ds:\\n            var_diff += ((img - mu)**2).sum()\\n        N = len(train_ds) * np.prod(np.array(img.shape))\\n        std = np.sqrt(var_diff / (N - 1))\\n\\n        self.mean = mu.item()\\n        self.std = std.item()\\n        train_ds.set_image_stats(*self.image_stats())\\n        train_ds.set_normalisation(True)\\n        self.train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4)\\n        self.valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=4)\\n        if test_size > 0:\\n            test_filenames = random_list[train_size + valid_size:]\\n            test_ds = SegmentationDataset(image_dir, mask_dir, test_filenames,\\n                transform)\\n            self.test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=4)\\n        else:\\n            self.test_dl = None\\n    \\n    def image_stats(self):\\n        # This needs to be stored somewhere\\n        return 0.6969700455665588, 13.313282012939453\\n\\ndef count_classes(dl, num_classes):\\n    count = {key: 0 for key in range(num_classes)}\\n    for batch in dl:\\n        _, truth = batch\\n        unique, counts = torch.unique(truth, return_counts=True)\\n        unique = [ u.item() for u in unique ]\\n        counts = [ c.item() for c in counts ]\\n        this_dict = dict(zip(unique, counts))\\n        for key in this_dict:\\n                count[key] += this_dict[key]\\n    return count\\n',\n"," 'img_util.py': b'import numpy as np\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.colors import ListedColormap, BoundaryNorm\\n\\ndef imagify(inputs, predictions, masks, void_code, n=3, randomize=True):\\n    \"\"\"Process input, prediction and mask data ready for display\\n    Args:\\n        inputs (np.ndarray): Input tensor from a batch.\\n        predictions (np.ndarray): Predictions tensor from a batch.\\n        masks (np.ndarray): Mask tensor from a batch.\\n        void_code (int): The null mask code (typically zero).\\n        n (int): The number of images to extract from the batch.\\n        randomize (boolean): If True choose a random set of images from the\\n            batch, otherwise pick the first n.\\n    Returns:\\n        A zip of the three processed images ready for display.\\n    \"\"\"\\n    # Select the images to process\\n    if randomize:\\n        choices = np.random.choice(np.array(range(inputs.shape[0])), size=n)\\n    else:\\n        choices = np.array(range(n))\\n    # Subset the inputs and masks\\n    input_imgs = inputs[choices,0,...]\\n    mask_imgs = masks[choices,...]\\n\\n    # Create a void code mask, determine the class of each predicted pixel and\\n    # then apply the mask to remove non-hit regions\\n    msks = mask_imgs == void_code\\n    pred_imgs = np.argmax(predictions[choices,...], axis=1)\\n    pred_imgs = np.ma.array(pred_imgs, mask = msks).filled(0)\\n    if n > 1:\\n        return zip(pred_imgs, mask_imgs)\\n    else:\\n        return pred_imgs, mask_imgs\\n    #return zip(input_imgs, pred_imgs, mask_imgs)\\n\\ndef show_batch(epoch, batch, inputs, predictions, masks, void_code, is_training, n=3, randomize=True):\\n    \"\"\"Display the images for a given epoch and batch. Each row is a triplet of\\n        input, prediction and mask.\\n\\n    Args:\\n        epoch (int): The current training epoch.\\n        batch (int): The current training batch.\\n        inputs (np.ndarray): Input tensor from a batch.\\n        predictions (np.ndarray): Predictions tensor from a batch.\\n        masks (np.ndarray): Mask tensor from a batch.\\n        void_code (int): The null mask code (typically zero).\\n        n (int): The number of images to extract from the batch.\\n        randomize (boolean): If True choose a random set of images from the\\n            batch, otherwise pick the first n.\\n    \"\"\"\\n    ax = None\\n    rows, cols = 1, 2\\n    size = 9\\n\\n    cmap = ListedColormap([\\'black\\', \\'red\\', \\'blue\\'])\\n    norm = BoundaryNorm([0., 0.5, 1.5, 2.], cmap.N)\\n\\n    xtr = dict(cmap=cmap, norm=norm, alpha=1.0)\\n\\n    images = imagify(inputs, predictions, masks, void_code, n, randomize)\\n\\n    for i, imgs in enumerate(images):\\n        # Produce output with 10% probability\\n        #if torch.rand(1).item() > 0.1: continue\\n        fig, axs = plt.subplots(1, cols, figsize=(cols * size, size))\\n        for img, ax in zip(imgs, axs):\\n            ax.imshow(img, **xtr)\\n            ax.axis(\\'off\\')\\n        plt.tight_layout()\\n        if is_training: save_figure(plt, \"training_{}_{}_{}\".format(epoch, batch, i))\\n        else: save_figure(plt, \"validation_{}_{}_{}\".format(epoch, batch, i))\\n        plt.close(fig)\\n\\ndef save_figure(fig, name):\\n    \"\"\"Output a matplotlib figure PNG, PDF and EPS formats.\\n\\n    Args:\\n        fig (Figure): The matplotlib figure to save.\\n        name (str): The output filename excluding extension.\\n    \"\"\"\\n    fig.savefig(name + \".png\")\\n    fig.savefig(name + \".pdf\")\\n    fig.savefig(name + \".eps\")\\n\\ndef get_supported_formats():\\n    \"\"\"Output a matplotlib figure PNG, PDF and EPS formats.\\n\\n    Args:\\n\\n    Returns:\\n        A dictionary containing strings of file format descriptions keyed by\\n            extension.\\n    \"\"\"\\n    return plt.gcf().canvas.get_supported_filetypes()\\n',\n"," 'model_util.py': b'import numpy as np\\nfrom torch.optim.lr_scheduler import LambdaLR\\nfrom torch.autograd import Variable\\n\\ndef get_exponential_factor(start, stop, n, epoch):\\n    ratio = stop / start\\n    factor = ratio ** (1. / (n - 1))\\n    return factor ** epoch\\n\\ndef get_linear_factor(start, stop, n, epoch):\\n    target = start + (epoch / (n - 1.)) * (stop - start)\\n    return target / start\\n\\nclass LRFinder():\\n    def __init__(self, model, bunch, loss_fn, optim, num_iter, void_code,\\n                 low_lr=1e-7, high_lr=2, is_exponential=True):\\n        self.num_iter = num_iter\\n        if is_exponential:\\n            self.lambda_func = lambda batch : get_exponential_factor(low_lr, high_lr, self.num_iter, batch)\\n        else:\\n            self.lambda_func = lambda batch : get_linear_factor(low_lr, high_lr, self.num_iter, batch)\\n        self.scheduler = LambdaLR(optim, lr_lambda=self.lambda_func)\\n        self.learner = UNetLearner(model, bunch, loss_fn, optim, self.scheduler, void_code)\\n    \\n    def find(self):\\n        num_epochs = int(np.floor(self.num_iter/len(self.learner.bunch.train_dl)))\\n        for epoch in list(range(num_epochs)):\\n            # This is ugly, finder needs to know too much about learner\\n            train_dl = self.learner.bunch.train_dl\\n            for i, batch in enumerate(train_dl):\\n                self.learner.is_training = True\\n                self.learner._batch(epoch, i, batch)\\n                self.learner.evaluate(0)\\n    \\n    def get_lrs(self):\\n        return torch.Tensor(self.learner.history[\"lr\"]).cpu()\\n\\n    def get_losses(self):\\n        num_batches = len(self.learner.bunch.valid_dl)\\n        x = torch.Tensor(self.learner.history[\"val_loss\"])\\n        x = x.reshape(x.size(0) // num_batches, num_batches).mean(axis = 1)\\n        return x.cpu()\\n\\n    def get_accuracies(self):\\n        num_batches = len(self.learner.bunch.valid_dl)\\n        x = torch.Tensor(self.learner.history[\"val_acc\"])\\n        x = x.reshape(x.size(0) // num_batches, num_batches).mean(axis = 1)\\n        return x.cpu()\\n',\n"," 'transforms.py': b'import torch\\nimport numpy as np\\nfrom skimage import transform\\n\\nclass ToTensor(object):\\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\\n\\n    def __init__(self, is_categorical=False):\\n        self.is_categorical = is_categorical\\n\\n    def __call__(self, sample):\\n        image, mask = sample\\n\\n        image = torch.from_numpy(np.expand_dims(image, axis=0))\\n        if not self.is_categorical:\\n            mask = torch.from_numpy(np.expand_dims(mask, axis=0))\\n        else:\\n            mask = torch.from_numpy(mask)\\n\\n        return (image, mask)\\n\\nclass Rescale(object):\\n    \"\"\"Rescale the image in a sample to a given size.\\n\\n    Args:\\n        output_size (tuple or int): Desired output size. If tuple, output is\\n            matched to output_size. If int, smaller of image edges is matched\\n            to output_size keeping aspect ratio the same.\\n    \"\"\"\\n\\n    def __init__(self, output_size):\\n        assert isinstance(output_size, (tuple))\\n        self.output_size = output_size\\n\\n    def __call__(self, sample):\\n        image, mask = sample\\n\\n        new_h, new_w = self.output_size\\n        new_h, new_w = int(new_h), int(new_w)\\n\\n        img = transform.resize(image, (new_h, new_w))\\n        mask = transform.resize(mask, (new_h, new_w))\\n\\n        return (img, mask)\\n',\n"," 'unet.py': b'import torch.nn as nn\\nimport torch\\n\\ndef maxpool():\\n    return nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\\n\\ndef dropout(prob):\\n    return nn.Dropout(prob)\\n\\ndef reinit_layer(seq_block, leak = 0.0, use_kaiming_normal=True):\\n    for layer in seq_block:\\n        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.ConvTranspose2d):\\n            print(\"Reinitialising\", layer)\\n            if use_kaiming_normal:\\n                nn.init.kaiming_normal_(layer.weight, a = leak)\\n            else:\\n                nn.init.kaiming_uniform_(layer.weight, a = leak)\\n                layer.bias.data.zero_()\\n\\nclass InitBlock(nn.Module):\\n    def __init__(self, c_in, c_out):\\n        super(InitBlock, self).__init__()\\n        self.conv = nn.Conv2d(c_in, c_out, kernel_size = 7, stride = 2, padding = 3, bias = False)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.bn = nn.BatchNorm2d(c_out)\\n        self.pool = nn.MaxPool2d(3, stride = 2, padding = 1)\\n        self.block = nn.Sequential(self.conv, self.relu, self.bn)#, self.pool)\\n        reinit_layer(self.block, leak = 0.0)\\n\\n    def forward(self, x):\\n        return self.block(x)\\n\\nclass ResBlock(nn.Module):\\n    def __init__(self, c_in, c_out):\\n        super(ResBlock, self).__init__()\\n        if c_out != c_in:\\n            # In principle this should be stride 2 and no maxpool after\\n            self.conv1 = nn.Conv2d(c_in, c_out, kernel_size = 3, stride = 1, padding = 1, bias = False)\\n        else:\\n            self.conv1 = nn.Conv2d(c_in, c_out, kernel_size = 3, stride = 1, padding = 1, bias = False)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.bn1 = nn.BatchNorm2d(c_out)\\n        self.conv2 = nn.Conv2d(c_out, c_out, kernel_size = 3, padding = 1, stride = 1, bias = False)\\n        self.bn2 = nn.BatchNorm2d(c_out)\\n        self.block = nn.Sequential(self.conv1, self.bn1, self.relu, self.conv2, self.bn2)\\n        if c_out != c_in:\\n            # May need to become stride 2 if stride above is included again\\n            ds_conv = nn.Conv2d(c_in, c_out, kernel_size = 1, stride = 1, bias = False)\\n            ds_bn = nn.BatchNorm2d(c_out)\\n            self.downsample = nn.Sequential(ds_conv, ds_bn)\\n        else:\\n            self.downsample = None\\n        reinit_layer(self.block, leak = 0.0)\\n\\n    def forward(self, x):\\n        identity = x\\n        out = self.block(x)\\n        if self.downsample: identity = self.downsample(x)\\n        out += identity\\n        out = self.relu(out)\\n\\n        return out\\n\\nclass ConvBlock(nn.Module):\\n    # Sigmoid activation suitable for binary cross-entropy\\n    def __init__(self, c_in, c_out, k_size = 3, k_pad = 1):\\n        # 3, 1 v 5, 2\\n        super(ConvBlock, self).__init__()\\n        self.block = nn.Sequential(\\n            nn.Conv2d(c_in, c_out, kernel_size = k_size, padding = k_pad, stride = 1),\\n            nn.BatchNorm2d(c_out),\\n            nn.ReLU(inplace=True),\\n            nn.Conv2d(c_out, c_out, kernel_size = k_size, padding = k_pad, stride = 1),\\n            nn.BatchNorm2d(c_out))\\n        reinit_layer(self.block)\\n\\n    def forward(self, x):\\n        return self.block(x)\\n\\nclass ConvBlockOld(nn.Module):\\n    # Sigmoid activation suitable for binary cross-entropy\\n    def __init__(self, c_in, c_out):\\n        super(ConvBlock, self).__init__()\\n        self.block = nn.Sequential(\\n            nn.Conv2d(c_in, c_out, kernel_size = 3, padding = 1, stride = 1),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(c_out),\\n            nn.Conv2d(c_out, c_out, kernel_size = 3, padding = 1, stride = 1),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(c_out))\\n        reinit_layer(self.block)\\n\\n    def forward(self, x):\\n        return self.block(x)\\n\\nclass TransposeConvBlock(nn.Module):\\n    # Sigmoid activation suitable for binary cross-entropy\\n    def __init__(self, c_in, c_out, k_size = 3, k_pad = 1):\\n        # 3, 1 v 5,2\\n        super(TransposeConvBlock, self).__init__()\\n        self.block = nn.Sequential(\\n            nn.ConvTranspose2d(c_in, c_out, kernel_size = k_size, padding = k_pad, output_padding = 1, stride = 2),\\n            nn.BatchNorm2d(c_out),\\n            nn.ReLU(inplace=True))\\n        reinit_layer(self.block)\\n\\n    def forward(self, x):\\n        return self.block(x)\\n\\nclass TransposeConvBlockOld(nn.Module):\\n    # Sigmoid activation suitable for binary cross-entropy\\n    def __init__(self, c_in, c_out):\\n        super(TransposeConvBlock, self).__init__()\\n        self.block = nn.Sequential(\\n            nn.ConvTranspose2d(c_in, c_out, kernel_size = 3, padding = 1, output_padding = 1, stride = 2),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(c_out))\\n        reinit_layer(self.block)\\n\\n    def forward(self, x):\\n        return self.block(x)\\n\\nclass Sigmoid(nn.Module):\\n    # Sigmoid activation suitable for binary cross-entropy\\n    def __init__(self):\\n        super(Sigmoid, self).__init__()\\n    \\n    def forward(self, x):\\n        return torch.sigmoid(x)\\n\\nclass SigmoidRange(nn.Module):\\n    # Sigmoid activation suitable for categorical cross-entropy\\n    def __init__(self, low, high):\\n        super(SigmoidRange, self).__init__()\\n        self.low = low\\n        self.high = high\\n    \\n    def forward(self, x):\\n        return torch.sigmoid(x) * (self.high - self.low) + self.low\\n\\nclass ListModule(nn.Module):\\n    def __init__(self, *args):\\n        super(ListModule, self).__init__()\\n        for i, module in enumerate(args):\\n            self.add_module(str(i), module)\\n\\n    def __getitem__(self, idx):\\n        if idx < 0 or idx >= len(self._modules):\\n            raise IndexError(\\'index {} is out of range\\'.format(idx))\\n        it = iter(self._modules.values())\\n        for i in range(idx):\\n            next(it)\\n        return next(it)\\n\\n    def __iter__(self):\\n        return iter(self._modules.values())\\n\\n    def __len__(self):\\n        return len(self._modules)\\n\\nclass UNet(nn.Module):\\n    def __init__(self, in_dim, n_classes, depth = 4, n_filters = 16, drop_prob = 0.1, y_range = None):\\n        super(UNet, self).__init__()\\n        # Contracting Path\\n        ds_convs = []\\n        for i in range(depth):\\n            if i == 0: ds_convs.append(ConvBlock(in_dim, n_filters * 2**i))\\n            else: ds_convs.append(ConvBlock(n_filters * 2**(i - 1), n_filters * 2**i))\\n        self.ds_convs = ListModule(*ds_convs)\\n\\n        ds_maxpools = []\\n        for i in range(depth):\\n            ds_maxpools.append(maxpool())\\n        self.ds_maxpools = ListModule(*ds_maxpools)\\n        \\n        ds_dropouts = []\\n        for i in range(depth):\\n            ds_dropouts.append(dropout(drop_prob))\\n        #self.ds_dropouts = ListModule(*ds_dropouts)\\n\\n        self.bridge = ConvBlock(n_filters * 2**(depth - 1), n_filters * 2**depth)\\n        \\n        # Expansive Path\\n        us_tconvs = []\\n        for i in range(depth, 0, -1):\\n            us_tconvs.append(TransposeConvBlock(n_filters * 2**i, n_filters * 2**(i - 1)))\\n        self.us_tconvs = ListModule(*us_tconvs)\\n\\n        us_convs = []\\n        for i in range(depth, 0, -1):\\n            us_convs.append(ConvBlock(n_filters * 2**i, n_filters * 2**(i - 1)))\\n        self.us_convs = ListModule(*us_convs)\\n\\n        us_dropouts = []\\n        for i in range(depth):\\n            us_dropouts.append(dropout(drop_prob))\\n        #self.us_dropouts = ListModule(*us_dropouts)\\n\\n        # Assume 3 classes to figure out\\n        if y_range is not None:\\n            self.output = nn.Sequential(\\n                nn.Conv2d(n_filters * 1, n_classes, 1),\\n                #Sigmoid(),\\n                SigmoidRange(*y_range)\\n            )\\n        else:\\n            self.output = nn.Sequential(\\n                nn.Conv2d(n_filters * 1, n_classes, 1)\\n            )\\n\\n    def forward(self, x):\\n        res = x\\n        conv_stack = []\\n\\n        # Downsample\\n        for i in range(len(self.ds_convs)):\\n            res = self.ds_convs[i](res); conv_stack.append(res)\\n            res = self.ds_maxpools[i](res)\\n            #res = self.ds_dropouts[i](res)\\n        \\n        # Bridge\\n        res = self.bridge(res)\\n        \\n        # Upsample\\n        for i in range(len(self.us_convs)):\\n            res = self.us_tconvs[i](res)\\n            res = torch.cat([res, conv_stack.pop()], dim=1)\\n            #res = self.us_dropouts[i](res)\\n            res = self.us_convs[i](res)\\n\\n        output = self.output(res)\\n        #print(output.size())\\n        return output\\n',\n"," 'unet_learner.py': b'SHOWER = 1\\nTRACK = 2\\n\\ndef switch_batch_norm(model, track_running_stats):\\n    for group in model.children():\\n        if type(group) == nn.Sequential:\\n            for child in group:\\n                if type(child) == nn.BatchNorm2d:\\n                    child.track_running_stats = track_running_stats\\n\\ndef summarize_epoch(history, n_train_batches, n_valid_batches):\\n    epoch = len(history[\"train_loss\"]) // n_train_batches\\n    train_loss = torch.Tensor(history[\"train_loss\"][-n_train_batches:]).mean()\\n    train_acc = torch.Tensor(history[\"train_acc\"][-n_train_batches:]).mean()\\n    valid_loss = torch.Tensor(history[\"val_loss\"][-n_valid_batches:]).mean()\\n    valid_acc = torch.Tensor(history[\"val_acc\"][-n_valid_batches:]).mean()\\n    valid_acc_shower = torch.Tensor(history[\"val_acc_shower\"][-n_valid_batches:]).mean()\\n    valid_acc_track = torch.Tensor(history[\"val_acc_track\"][-n_valid_batches:]).mean()\\n\\n    print(\"Epoch {} : Training Loss {:.3f} Acc {:.3f} Validation Loss {:.3f} \" \\\\\\n          \"Acc {:.3f} T Acc {:.3f} S Acc {:.3f}\".format(epoch, train_loss,\\n          train_acc, valid_loss, valid_acc, valid_acc_track, valid_acc_shower))\\n\\ndef save_network(model, input, filename):\\n    eval_model = model.eval()\\n    torch.save(eval_model.state_dict(), f\"{filename}.pkl\")\\n    # Load later with:\\n    # model.load_state_dict(torch.load(PATH))\\n    # model.eval()\\n\\n    sm = torch.jit.trace(eval_model, input)\\n    sm.save(f\"{filename}_traced.pt\")\\n\\n    #sm = torch.jit.script(model)\\n    #sm.save(f\"{filename}_script.pt\")\\n\\nclass UNetLearner:\\n    def __init__(self, model, bunch, loss_fn, optim, scheduler, void_code=0):\\n        self.model = model\\n        self.bunch = bunch\\n        self.loss_fn = loss_fn\\n        self.optim = optim\\n        self.scheduler = scheduler\\n        self.void_code = void_code\\n        self.is_training = True\\n        self.verbose = 0\\n        self.history = {\"lr\" : [], \"train_loss\" : [], \"train_acc\" : [],\\n                        \"train_acc_shower\" : [], \"train_acc_track\" : [],\\n                        \"val_loss\" : [], \"val_acc\" : [],\\n                        \"val_acc_shower\" : [], \"val_acc_track\" : []}\\n\\n    def train(self, n_epochs):\\n        self.model = self.model.cuda()\\n        for e in range(n_epochs):\\n            # Train\\n            self.is_training = True\\n            self.model = self.model.train()\\n            self._epoch(e)\\n            # Save the network\\n            for batch in self.bunch.valid_dl:\\n                example, _ = batch\\n                example = Variable(example).cuda()\\n                save_network(model, example, f\"unet_{e}\")\\n\\n            # Validate\\n            self.model = self.model.eval()\\n            switch_batch_norm(self.model, False)\\n            with torch.no_grad():\\n                self.evaluate(e)\\n            switch_batch_norm(self.model, True)\\n            summarize_epoch(self.history, len(self.bunch.train_dl),\\n                            len(bunch.valid_dl))\\n    \\n    def evaluate(self, e):\\n        self.is_training = False\\n        self._epoch(e)\\n\\n    def _batch(self, e, b, batch):\\n        images, truth = batch\\n        x = Variable(images).cuda()\\n        y = Variable(truth).cuda()\\n        pred = self.model.forward(x)\\n        loss = self.loss_fn(pred, y)\\n\\n        key = \"train\" if self.is_training else \"val\"\\n        if self.is_training:\\n            self.history[\"lr\"].append(self.scheduler.get_lr()[0])\\n        self.history[f\"{key}_loss\"].append(loss.item())\\n        self.history[f\"{key}_acc\"].append(self.accuracy(pred, y))\\n        self.history[f\"{key}_acc_track\"].append(self.accuracy(pred, y, TRACK))\\n        self.history[f\"{key}_acc_shower\"].append(self.accuracy(pred, y, SHOWER))\\n\\n        if not self.is_training and self.verbose > 0:\\n            if self.verbose > 1: #and b == (len(self.bunch.valid_dl) - 1):\\n                net_input = x.cpu().detach().numpy()\\n                net_pred = pred.cpu().detach().numpy()\\n                net_mask = y.cpu().detach().numpy()\\n                show_batch(e, b, net_input, net_pred, net_mask, self.void_code,\\n                            self.is_training, n = images.shape[0], randomize=False)\\n                label = \"Training\" if self.is_training else \"Validation\"\\n                print(\"Batch {}: {} Loss: {:.3f} Acc: {:.3f} S Acc: {:.3f} T Acc: {:.3f}\".format(\\n                    b + 1, label, loss.item(), self.history[f\"{key}_acc\"][-1],\\n                    self.history[f\"{key}_acc_shower\"][-1], self.history[f\"{key}_acc_track\"][-1]))\\n            elif e == 4 and b == 0:    # Should add epochs to self on call to train\\n                net_input = x.cpu().detach().numpy()\\n                net_pred = pred.cpu().detach().numpy()\\n                net_mask = y.cpu().detach().numpy()\\n                #iu.show_batch(e, b, net_input, net_pred, net_mask, self.void_code,\\n                #            self.is_training, n = images.shape[0], randomize=False)\\n                show_batch(e, b, net_input, net_pred, net_mask, self.void_code,\\n                            self.is_training, n = images.shape[0], randomize=False)\\n                label = \"Training\" if self.is_training else \"Validation\"\\n                print(\"Batch {}: {} Loss: {:.3f} Acc: {:.3f} S Acc: {:.3f} T Acc: {:.3f}\".format(\\n                    b + 1, label, loss.item(), self.history[f\"{key}_acc\"][-1],\\n                    self.history[f\"{key}_acc_shower\"][-1], self.history[f\"{key}_acc_track\"][-1]))\\n            elif e == 10:\\n                net_input = x.cpu().detach().numpy()\\n                net_pred = pred.cpu().detach().numpy()\\n                net_mask = y.cpu().detach().numpy()\\n                label = \"Training\" if self.is_training else \"Validation\"\\n                print(\"Batch {}: {} Loss: {:.3f} Acc: {:.3f} S Acc: {:.3f} T Acc: {:.3f}\".format(\\n                    b + 1, label, loss.item(), self.history[f\"{key}_acc\"][-1],\\n                    self.history[f\"{key}_acc_shower\"][-1], self.history[f\"{key}_acc_track\"][-1]))\\n            #elif e == 4:\\n            #    net_input = x.cpu().detach().numpy()\\n            #    net_pred = pred.cpu().detach().numpy()\\n            #    net_mask = y.cpu().detach().numpy()\\n            #    show_batch(e, b, net_input, net_pred, net_mask, self.void_code,\\n            #                self.is_training, n = images.shape[0], randomize=False)\\n\\n        if self.is_training:\\n            loss.backward()\\n            self.optim.step()\\n            self.scheduler.step()\\n            self.optim.zero_grad()\\n\\n    def _epoch(self, epoch):\\n        dl = self.bunch.train_dl if self.is_training else self.bunch.valid_dl\\n        for i, batch in enumerate(dl):\\n            self._batch(epoch, i, batch)\\n    \\n    def set_verbose(self, verbose):\\n        self.verbose = verbose\\n\\n    def accuracy(self, input, truth, type=None):\\n        target = truth.squeeze(1)\\n        if type is None:\\n            mask = target != self.void_code\\n        else:\\n            mask = target == type\\n        return (input.argmax(dim=1)[mask] == target[mask]).float().mean()\\n'}"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"Lm-SwXGXmASL","colab_type":"code","colab":{}},"source":["from data import *\n","from transforms import *\n","from unet import *\n","from unet_learner import *\n","from img_util import *\n","from model_util import *"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9oRKrIDaEJW6","colab_type":"text"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"26Oon8j0Yqxa","colab_type":"code","colab":{}},"source":["from torchvision import transforms\n","from torch.autograd import Variable\n","import torch\n","import torch.optim as opt\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vUJs_mmpY7Qt","colab_type":"code","colab":{}},"source":["set_seed(the_seed)\n","batch_size=96\n","bunch = SegmentationBunch(\"Images\", \"Hits\", \"Truth\", batch_size=batch_size, valid_pct = 0.3,\n","    transform=transforms.Compose([ToTensor(True)]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GaiVF0fMI7Uf","colab_type":"code","outputId":"d897d728-9aec-4aee-f6de-9e81197b77ec","executionInfo":{"status":"ok","timestamp":1581321875087,"user_tz":0,"elapsed":27933,"user":{"displayName":"Andy Chappell","photoUrl":"https://lh4.googleusercontent.com/-TuFiExc5DlA/AAAAAAAAAAI/AAAAAAAAIWA/grw-7ZxGJac/s64/photo.jpg","userId":"00288272189243102020"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def get_class_weights(n_track, n_shower, n_void):\n","    weights = [1. / n_void, 1. / n_shower, 1. / n_track]\n","    return [weight / sum(weights) for weight in weights]\n","\n","n_track = 1114287\n","n_shower = 516073\n","n_void = 684138344\n","\n","weights = get_class_weights(n_track, n_shower, n_void)\n","weights"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.0005152961649927253, 0.6831085233826877, 0.3163761804523196]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"iivejy-O2VCT","colab_type":"code","colab":{}},"source":["# Implement OneCycleLR class vailable in later versions of torch\n","from torch.optim import Optimizer\n","from torch.optim.lr_scheduler import _LRScheduler\n","import math\n","class OneCycleLR(_LRScheduler):\n","    def __init__(self,\n","                 optimizer,\n","                 max_lr,\n","                 total_steps=None,\n","                 epochs=None,\n","                 steps_per_epoch=None,\n","                 pct_start=0.3,\n","                 anneal_strategy='cos',\n","                 cycle_momentum=True,\n","                 base_momentum=0.85,\n","                 max_momentum=0.95,\n","                 div_factor=25.,\n","                 final_div_factor=1e4,\n","                 last_epoch=-1):\n","\n","        # Validate optimizer\n","        if not isinstance(optimizer, Optimizer):\n","            raise TypeError('{} is not an Optimizer'.format(\n","                type(optimizer).__name__))\n","        self.optimizer = optimizer\n","\n","        # Validate total_steps\n","        if total_steps is None and epochs is None and steps_per_epoch is None:\n","            raise ValueError(\"You must define either total_steps OR (epochs AND steps_per_epoch)\")\n","        elif total_steps is not None:\n","            if total_steps <= 0 or not isinstance(total_steps, int):\n","                raise ValueError(\"Expected non-negative integer total_steps, but got {}\".format(total_steps))\n","            self.total_steps = total_steps\n","        else:\n","            if epochs <= 0 or not isinstance(epochs, int):\n","                raise ValueError(\"Expected non-negative integer epochs, but got {}\".format(epochs))\n","            if steps_per_epoch <= 0 or not isinstance(steps_per_epoch, int):\n","                raise ValueError(\"Expected non-negative integer steps_per_epoch, but got {}\".format(steps_per_epoch))\n","            self.total_steps = epochs * steps_per_epoch\n","        self.step_size_up = float(pct_start * self.total_steps) - 1\n","        self.step_size_down = float(self.total_steps - self.step_size_up) - 1\n","\n","        # Validate pct_start\n","        if pct_start < 0 or pct_start > 1 or not isinstance(pct_start, float):\n","            raise ValueError(\"Expected float between 0 and 1 pct_start, but got {}\".format(pct_start))\n","\n","        # Validate anneal_strategy\n","        if anneal_strategy not in ['cos', 'linear']:\n","            raise ValueError(\"anneal_strategy must by one of 'cos' or 'linear', instead got {}\".format(anneal_strategy))\n","        elif anneal_strategy == 'cos':\n","            self.anneal_func = self._annealing_cos\n","        elif anneal_strategy == 'linear':\n","            self.anneal_func = self._annealing_linear\n","\n","        # Initialize learning rate variables\n","        max_lrs = self._format_param('max_lr', self.optimizer, max_lr)\n","        if last_epoch == -1:\n","            for idx, group in enumerate(self.optimizer.param_groups):\n","                group['initial_lr'] = max_lrs[idx] / div_factor\n","                group['max_lr'] = max_lrs[idx]\n","                group['min_lr'] = group['initial_lr'] / final_div_factor\n","\n","        # Initialize momentum variables\n","        self.cycle_momentum = cycle_momentum\n","        if self.cycle_momentum:\n","            if 'momentum' not in self.optimizer.defaults and 'betas' not in self.optimizer.defaults:\n","                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')\n","            self.use_beta1 = 'betas' in self.optimizer.defaults\n","            max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\n","            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)\n","            if last_epoch == -1:\n","                for m_momentum, b_momentum, group in zip(max_momentums, base_momentums, optimizer.param_groups):\n","                    if self.use_beta1:\n","                        _, beta2 = group['betas']\n","                        group['betas'] = (m_momentum, beta2)\n","                    else:\n","                        group['momentum'] = m_momentum\n","                    group['max_momentum'] = m_momentum\n","                    group['base_momentum'] = b_momentum\n","\n","        super(OneCycleLR, self).__init__(optimizer, last_epoch)\n","\n","    def _format_param(self, name, optimizer, param):\n","        \"\"\"Return correctly formatted lr/momentum for each param group.\"\"\"\n","        if isinstance(param, (list, tuple)):\n","            if len(param) != len(optimizer.param_groups):\n","                raise ValueError(\"expected {} values for {}, got {}\".format(\n","                    len(optimizer.param_groups), name, len(param)))\n","            return param\n","        else:\n","            return [param] * len(optimizer.param_groups)\n","\n","    def _annealing_cos(self, start, end, pct):\n","        \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n","        cos_out = math.cos(math.pi * pct) + 1\n","        return end + (start - end) / 2.0 * cos_out\n","\n","    def _annealing_linear(self, start, end, pct):\n","        \"Linearly anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n","        return (end - start) * pct + start\n","\n","    def get_lr(self):\n","        lrs = []\n","        step_num = self.last_epoch\n","\n","        if step_num > self.total_steps:\n","            raise ValueError(\"Tried to step {} times. The specified number of total steps is {}\"\n","                             .format(step_num + 1, self.total_steps))\n","\n","        for group in self.optimizer.param_groups:\n","            if step_num <= self.step_size_up:\n","                computed_lr = self.anneal_func(group['initial_lr'], group['max_lr'], step_num / self.step_size_up)\n","                if self.cycle_momentum:\n","                    computed_momentum = self.anneal_func(group['max_momentum'], group['base_momentum'],\n","                                                         step_num / self.step_size_up)\n","            else:\n","                down_step_num = step_num - self.step_size_up\n","                computed_lr = self.anneal_func(group['max_lr'], group['min_lr'], down_step_num / self.step_size_down)\n","                if self.cycle_momentum:\n","                    computed_momentum = self.anneal_func(group['base_momentum'], group['max_momentum'],\n","                                                         down_step_num / self.step_size_down)\n","\n","            lrs.append(computed_lr)\n","            if self.cycle_momentum:\n","                if self.use_beta1:\n","                    _, beta2 = group['betas']\n","                    group['betas'] = (computed_momentum, beta2)\n","                else:\n","                    group['momentum'] = computed_momentum\n","\n","        return lrs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVTDrZbtY3wl","colab_type":"code","outputId":"5b9726f2-2b92-4b38-8ec1-78af02b2d147","executionInfo":{"status":"ok","timestamp":1581321876451,"user_tz":0,"elapsed":826,"user":{"displayName":"Andy Chappell","photoUrl":"https://lh4.googleusercontent.com/-TuFiExc5DlA/AAAAAAAAAAI/AAAAAAAAIWA/grw-7ZxGJac/s64/photo.jpg","userId":"00288272189243102020"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"source":["# Create the network, loss function and optimizer\n","from torch.optim.lr_scheduler import OneCycleLR, LambdaLR\n","set_seed(the_seed)\n","num_epochs = 10\n","pct_start = 2. / num_epochs\n","n_classes = 3\n","wd = 1e-4\n","max_lr = 1e-2\n","base_lr = 1e-4\n","min_lr = 1e-7\n","div_factor = max_lr / base_lr\n","final_factor = base_lr / min_lr\n","model = UNet(1, n_classes = n_classes, depth = 4, n_filters = 16, y_range = (0, n_classes))\n","model.load_state_dict(torch.load(\"unet_9.pkl\", map_location=\"cpu\"))\n","model.eval()\n","loss_fn = nn.CrossEntropyLoss(torch.tensor(weights))\n","optim = opt.Adam(model.parameters(), lr=max_lr, weight_decay=wd)\n","scheduler = OneCycleLR(optim, max_lr, anneal_strategy='cos', total_steps = num_epochs * len(bunch.train_dl), pct_start=pct_start, div_factor=div_factor, final_div_factor=final_factor)\n","learner = UNetLearner(model, bunch, loss_fn, optim, scheduler)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Reinitialising Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","Reinitialising ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","Reinitialising ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","Reinitialising ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","Reinitialising Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Reinitialising Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p__WLNcBTBWQ","colab_type":"text"},"source":["# Create traced script module"]},{"cell_type":"code","metadata":{"id":"3dUUtcbDbi_D","colab_type":"code","colab":{}},"source":["set_seed(the_seed)\n","datum = None\n","for batch in learner.bunch.valid_dl:\n","    image, truth = batch\n","    x = Variable(image)\n","    y = Variable(truth)\n","    break\n","sm = torch.jit.trace(learner.model, x)\n","sm.save(\"unet_9_PyT_v1.4.0.pt\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wpJM_LkpwJkk","colab_type":"code","colab":{}},"source":["files.download(\"unet_9_PyT_v1.4.0.pt\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"frTt8z515nQO","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}